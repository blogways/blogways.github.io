I"j/<h2 id="一网络爬虫">一、网络爬虫</h2>
<p>网络爬虫或者叫网络机器人是用来自动采集网络内容的工具，通常被用于搜索引擎。基于我自己的理解，要写一个网络爬虫通常要解决以下几个问题：</p>

<ul>
  <li>有哪些网络资源是需要被爬取的。网络上的资源多种多样，有单纯的HTML页面，也有各种格式的文件、音频或视频等等，在本文中我仅爬取HTML页面中的文本，所以这里的首要问题是哪些URL是爬虫要处理的。</li>
  <li>如何从文本中提取到我想要的信息。HTML文本中包含了很多无用的信息，包括标签等，网络爬虫最终提供给我的应该是我感兴趣的内容。</li>
  <li>如何把爬取到的信息保存下来。爬虫得到的有用信息要通过数据库或其他有效途径保存下来。</li>
  <li>其他问题：包括网站的反爬虫策略、爬虫策略、错误处理能力等等。</li>
</ul>

<p>以上几个问题如果单独提出来可能大家都有解决的办法，但是整合到一起处理，还是有一些麻烦的。Scrapy这个爬虫框架就提供给我们一个简单的解决方案。</p>

<h2 id="二scrapy">二、Scrapy</h2>
<p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架。它包含了几大组件：</p>

<ul>
  <li>Scrapy engine：它负责数据流在系统中所有组件中流动，相当于爬虫的“大脑”，是调度中心。</li>
  <li>Downloader：负责获取页面并提供给引擎。</li>
  <li>Spider：是真正的爬虫，负责URL和解析HTML提供item。</li>
  <li>Pipeline：负责处理spider提供的item。典型的应用有清理错误、持久化等。</li>
  <li>Middlewares：处理spider的输入和输出。典型的应用有网络代理验证。</li>
</ul>

<p><img src="/images/jyjsjd/scrapy_workflow.png" alt="scrapy_workflow.png" /></p>

<h2 id="三新建一个项目">三、新建一个项目</h2>
<p>在写爬虫之前我们要新建一个项目，Scrapy非常贴心的为我们提供了一个命令帮助我们建立项目框架：</p>

<p><code class="language-plaintext highlighter-rouge">scrapy startproject [project_name]</code></p>

<p>这个命令建立的项目框架如下：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>project_name/
  scrapy.cfg
  db/
    __init__.py
    config.py
  spiders/
    __init__.py
    spider.py
  items.py
  pipelines.py
  settings.py
  spiders/
      __init__.py
      ...
</code></pre></div></div>

<ul>
  <li>scrapy.cfg: 项目的配置文件。</li>
  <li>db/config.py：数据库的设置文件。</li>
  <li>spiders/：爬虫所在的目录。</li>
  <li>project_name/items.py: 项目中的item文件。</li>
  <li>project_name/pipelines.py: 项目中的pipelines文件。</li>
  <li>project_name/settings.py: 项目的设置文件。包括数据库设置等。</li>
  <li>project_name/spiders/: 放置spider代码的目录。</li>
</ul>

<h2 id="四编写一个爬虫">四、编写一个爬虫</h2>
<p>房子这两年大热，这里我写一个爬取链家网二手房的爬虫为例。打开<a href="http://nj.lianjia.com/ershoufang/">网页</a>来观察一下结构：</p>

<p><img src="/images/jyjsjd/page_example.png" alt="page_example.png" /></p>

<p>网页看上去十分规整，包含了很多我们想要的信息，包括总价、单价、面积和位置等。</p>

<h3 id="声明item">声明item</h3>
<p>Item是spider得到的结构化数据。</p>

<p>打开items.py，用Python简单的class定义语法来新建一个类：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import scrapy
from scrapy.loader.processors import MapCompose, TakeFirst

class HomeLinkItem(scrapy.Item):
    hid = scrapy.Field()
    url = scrapy.Field()
    title = scrapy.Field()
    price = scrapy.Field()
    room = scrapy.Field()
    htype = scrapy.Field()
    area = scrapy.Field()
    areaName = scrapy.Field()
</code></pre></div></div>

<h3 id="爬虫">爬虫</h3>
<p>在<code class="language-plaintext highlighter-rouge">spiders/</code>目录下新建home_link.py，作为爬虫。</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class HomeSpider(CrawlSpider):
    name = "home_link"

    def parse_item(self, response):
        ...
</code></pre></div></div>

<p>爬虫要知道从哪里爬取，所以第一步是告诉爬虫域名和开始的URL：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>allow_domains = ['nj.lianjia.com']
start_urls = ['http://nj.lianjia.com/ershoufang/']
</code></pre></div></div>

<p>爬虫开始抓取网页之后还有告诉爬虫哪些URL是要处理的，也就是爬取的规则：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>rules = (
    Rule(LinkExtractor(allow=(r'\d+\.html', r'pg\d+\\')), callback='parse_item', follow=True),
)
</code></pre></div></div>

<p>大家可以打开刚才那个网页中的任意一个房子，观察它的URL。可以发现，URL都是域名加一串数字构成的。如：
  <code class="language-plaintext highlighter-rouge">http://nj.lianjia.com/ershoufang/103100443605.html</code>。</p>

<p>这里可以用正则表达式写多个规则，我在这里只写了一个。</p>

<h3 id="解析网页">解析网页</h3>
<p>爬虫中的<code class="language-plaintext highlighter-rouge">parse_item</code>方法就是用来解析网页，并返回一个item的。</p>

<p>Python有很多解析HTML的优秀库，如<code class="language-plaintext highlighter-rouge">BeautifulSoup</code>，我在这里专注于爬虫就不用这些库了。实际上我们可以非常简单地通过XPath语法来获取网页上我们感兴趣的内容。XPath非常简单，只要熟悉CSS选择器就可以快速上手。</p>

<p>我们再来看一下网页的源码，如我们感兴趣的<em>总价</em>：</p>

<p><img src="/images/jyjsjd/price_tag.png" alt="price_tag.png" /></p>

<p>可以看到<em>总价</em>位于<code class="language-plaintext highlighter-rouge">class</code>为<code class="language-plaintext highlighter-rouge">price</code>的<code class="language-plaintext highlighter-rouge">div</code>中的class为<code class="language-plaintext highlighter-rouge">total</code>的<code class="language-plaintext highlighter-rouge">div</code>中（为看起来方便没有截更多，实际上他们还被<code class="language-plaintext highlighter-rouge">class</code>为<code class="language-plaintext highlighter-rouge">content</code>的<code class="language-plaintext highlighter-rouge">div</code>包含）。</p>

<p>那么要取得总价，XPath写成：</p>

<p><code class="language-plaintext highlighter-rouge">//div[@class="content"]/div[@class="price "]/span[@class="total"]/text()</code></p>

<p>其他的字段都是类似我就不再赘述。</p>

<p>最终<code class="language-plaintext highlighter-rouge">parse_item</code>方法写成：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def parse_item(self, response):
    l = ItemLoader(item=HomeLinkItem(), response=response)
    l.add_xpath('hid', '//div[@class="content"]/span[@class="sharethis"]/span[@class="erweibox"]/img')
    l.add_value('url', response.url)
    l.add_xpath('title', '//title/text()')
    l.add_xpath('price', '//div[@class="content"]/div[@class="price "]/span[@class="total"]/text()')
    l.add_xpath('room', '//div[@class="content"]/div[@class="houseInfo"]/div[@class="room"]'
                        '/div[@class="mainInfo"]/text()')
    l.add_xpath('htype', '//div[@class="content"]/div[@class="houseInfo"]/div[@class="type"]'
                         '/div[@class="mainInfo"]/text()')
    l.add_xpath('area', '//div[@class="content"]/div[@class="houseInfo"]/div[@class="area"]'
                        '/div[@class="mainInfo"]/text()')
    l.add_xpath('areaName', '//div[@class="content"]/div[@class="aroundInfo"]/div[@class="communityName"]'
                            '/a[@class="info"]/text()')
    return l.load_item()
</code></pre></div></div>

<p>返回一个item。</p>

<h3 id="表的定义">表的定义</h3>
<p>最终我们想把数据保存在数据库中。Scrapy使用SQLAlchemy来帮助完成持久化，它相当于Hibernate，工作原理类似。</p>

<p>定义一个类<code class="language-plaintext highlighter-rouge">house</code>来保存数据，它对应数据库表<code class="language-plaintext highlighter-rouge">house</code>。sqlalchemy.Column()这句代码定义字段和它的类型等。
在db/下新建house.py：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import sqlalchemy
from sqlalchemy.ext.declarative import declarative_base

Base = declarative_base()

class House(Base):
    __tablename__ = 'house'

    hid = sqlalchemy.Column(sqlalchemy.String, primary_key=True, name='id')
    url = sqlalchemy.Column(sqlalchemy.String)
    title = sqlalchemy.Column(sqlalchemy.String)
    price = sqlalchemy.Column(sqlalchemy.String)
    room = sqlalchemy.Column(sqlalchemy.String)
    htype = sqlalchemy.Column(sqlalchemy.String, name='type')
    area = sqlalchemy.Column(sqlalchemy.String)
    areaName = sqlalchemy.Column(sqlalchemy.String, name='area_name')
</code></pre></div></div>

<p>数据库表<code class="language-plaintext highlighter-rouge">house</code>的定义如下：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>CREATE TABLE `house` (
  `id` varchar(30) NOT NULL DEFAULT '',
  `url` varchar(256) DEFAULT NULL,
  `title` varchar(200) DEFAULT NULL,
  `price` varchar(10) DEFAULT NULL,
  `room` varchar(100) DEFAULT NULL,
  `type` varchar(100) DEFAULT NULL,
  `area` varchar(50) DEFAULT NULL,
  `area_name` varchar(50) DEFAULT NULL,
  PRIMARY KEY (`id`)
) ENGINE=InnoDB DEFAULT CHARSET=utf8;
</code></pre></div></div>

<p>这里我使用的是MySQL，打开db/config.py，写入数据库连接字符串：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>engine = create_engine('mysql+mysqlconnector://user:pasword@localhost:3306/test')
DBSession = sessionmaker(bind=engine)
</code></pre></div></div>

<h3 id="保存结果">保存结果</h3>
<p>Pipelines的典型应用就是持久化数据。它接收Spider返回的item对象并对item进行处理。在这里我们把item包装成House类，并持久化。
打开<code class="language-plaintext highlighter-rouge">pipelines.py</code>，并加入以下内容：</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>class DataBasePipeline(object):
    def open_spider(self, spider):
        self.session = DBSession()

    def process_item(self, item, spider):
        h = House(
            hid=item.get('hid'),
            url=item.get('url'),
            title=item.get('title'),
            price=item.get('price'),
            room=item.get('room'),
            htype=item.get('htype'),
            area=item.get('area'),
            areaName=item.get('areaName')
        )
        self.session.add(h)
        self.session.commit()

    def close_spider(self, spider):
        self.session.close()
</code></pre></div></div>

<p><code class="language-plaintext highlighter-rouge">open_spider</code>方法取得数据库session。<code class="language-plaintext highlighter-rouge">process_item</code>方法处理对象，这里是将House对象存到数据库。</p>

<h3 id="运行爬虫">运行爬虫</h3>
<p>CD到项目目录，运行命令：<code class="language-plaintext highlighter-rouge">scrapy crawl home_link</code>。（home_link是爬虫的名字）</p>

<p><img src="/images/jyjsjd/scrapy_run.png" alt="scrapy_run.png" /></p>

<p>可以看到Scrapy取得了一个<code class="language-plaintext highlighter-rouge">house</code>。</p>
:ET